{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "\n",
    "The purpose of this notebook is to explore linear and logistic regression with reference to chapter 13 from *All of Statistics* (Wasserman, 2004)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Linear Regression\n",
    "\n",
    "**Regression** is a method for studying the relationship between a **response variable** $Y$ and a **covariate** $X$. The covariate is also called a **predictor variable** or a **feature**. One way to summarize the relationship between $X$ and $Y$ is through the regression function\n",
    "\n",
    "$$ r(x) = \\mathbb{E}(Y \\mid X = x) = \\int y \\, f(y \\mid x) \\, dy. $$\n",
    "\n",
    "Our goal is to estimate the regression function $r(x)$ from data of the form\n",
    "\n",
    "$$ (Y_1, X_1), \\ldots, (Y_n, X_n) \\sim F_{X,Y}. $$\n",
    "\n",
    "The simplest version of regression is when $X_i$ is simple (one-dimensional) and $r(x)$ is assumed to be linear:\n",
    "\n",
    "$$ r(x) = \\beta_0 + \\beta_1 x. $$\n",
    "\n",
    "This model is called the simple linear regression model. We will make the further simplifying assumption that $\\mathbb{V}(\\epsilon_i \\mid X = x) = \\sigma^2$ does not depend on $x$. We can thus write the **simple linear regression model** as\n",
    "\n",
    "$$ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i $$\n",
    "\n",
    "where $\\mathbb{E}(\\epsilon_i \\mid X_i) = 0$ and $\\mathbb{V}(\\epsilon_i \\mid X_i) = \\sigma^2$.\n",
    "\n",
    "<center><img src=\"../figures/simple_linear_reg.png\" width=\"350\" height=\"300\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unknown parameters in the model are the intercept $\\beta_0$ and the slope $\\beta_1$ and the variance $\\sigma^2$. Let $\\widehat{\\beta}_0$ and $\\widehat{\\beta}_1$ denote estimates of $\\beta_0$ and $\\beta_1$. The **fitted line** is\n",
    "\n",
    "$$ \\widehat{r}(x) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x. $$\n",
    "\n",
    "The **predicted values** or **fitted values** are $\\widehat{Y}_i = \\widehat{r}(X_i)$ and the **residuals** are defined to be\n",
    "\n",
    "$$ \\widehat{\\epsilon}_i = Y_i - \\widehat{Y}_i = Y_i - \\left(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_i\\right). $$\n",
    "\n",
    "The residual sums of squares or RSS, which measures how well the line fits the data, is defined by $\\text{RSS} = \\sum_{i=1}^n \\widehat{\\epsilon}_i^2$.\n",
    "\n",
    "The **least squares estimates** are the values $\\widehat{\\beta}_0$ and $\\widehat{\\beta}_1$ that minimize RSS. The least squares estimates are given by\n",
    "\n",
    "$$ \\widehat{\\beta}_1 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X}_n)(Y_i - \\bar{Y}_n)}{\\sum_{i=1}^n (X_i - \\bar{X}_n)^2} $$\n",
    "\n",
    "$$ \\widehat{\\beta}_0 = \\bar{Y}_n - \\widehat{\\beta}_1 \\bar{X}_n. $$\n",
    "\n",
    "An unbiased estimate of $\\sigma^2$ is\n",
    "\n",
    "$$ \\widehat{\\sigma}^2 = \\left(\\frac{1}{n - 2}\\right) \\sum_{i=1}^n \\widehat{\\epsilon}_i^2. $$\n",
    "\n",
    "Note: inferences from linear regression are most accurate when the residuals behave like random normal numbers.\n",
    "\n",
    "Let's consider an example in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "n = 100\n",
    "X = ss.uniform.rvs(loc=0, scale=10, size=n, random_state=42)\n",
    "true_beta0, true_beta1 = 2, 3\n",
    "true_var = 1\n",
    "epsilon = ss.norm.rvs(loc=0, scale=np.sqrt(true_var), size=n, random_state=42)\n",
    "Y = true_beta0 + true_beta1 * X + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True β0 = 2, Estimated β0 = 1.707\n",
      "True β1 = 3, Estimated β1 = 3.040\n",
      "Residual Sum of Squares (RSS) = 80.228\n",
      "True σ² = 1, Unbiased estimate of σ² = 0.819\n"
     ]
    }
   ],
   "source": [
    "# Compute least squares estimates\n",
    "X_mean, Y_mean = np.mean(X), np.mean(Y)\n",
    "ols_beta1 = np.sum((X - X_mean) * (Y - Y_mean)) / np.sum((X - X_mean)**2)\n",
    "ols_beta0 = Y_mean - ols_beta1 * X_mean\n",
    "\n",
    "# Compute fitted values and residuals\n",
    "ols_Y = ols_beta0 + ols_beta1 * X\n",
    "ols_resids = Y - ols_Y\n",
    "ols_RSS = np.sum(ols_resids**2)\n",
    "ols_var_hat = (1 / (n - 2)) * ols_RSS\n",
    "\n",
    "# Print results\n",
    "print(f\"True β0 = {true_beta0}, Estimated β0 = {ols_beta0:.3f}\")\n",
    "print(f\"True β1 = {true_beta1}, Estimated β1 = {ols_beta1:.3f}\")\n",
    "print(f\"Residual Sum of Squares (RSS) = {ols_RSS:.3f}\")\n",
    "print(f\"True σ² = {true_var}, Unbiased estimate of σ² = {ols_var_hat:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the data and the fitted line\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.scatter(X, Y, label='Data points')\n",
    "plt.plot(X, ols_Y, color='red', label='Regression line')\n",
    "plt.xlabel('X'), plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Least Squares and Maximum Likelihood\n",
    "\n",
    "Suppose we add the assumption that $\\epsilon_i \\mid X_i \\sim N(0, \\sigma^2)$, that is,\n",
    "\n",
    "$$ Y_i \\mid X_i \\sim N(\\mu_i, \\sigma^2) $$\n",
    "\n",
    "where $\\mu_i = \\beta_0 + \\beta_1 X_i$. The likelihood function is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\prod_{i=1}^n f(X_i, Y_i) &= \\prod_{i=1}^n f_X(X_i) f_{Y \\mid X}(Y_i \\mid X_i) \\\\\n",
    "&= \\prod_{i=1}^n f_X(X_i) \\times \\prod_{i=1}^n f_{Y \\mid X}(Y_i \\mid X_i) \\\\\n",
    "&= \\mathcal{L}_1 \\times \\mathcal{L}_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{L}_1 = \\prod_{i=1}^n f_X(X_i)$ and\n",
    "\n",
    "$$ \\mathcal{L}_2 = \\prod_{i=1}^n f_{Y \\mid X}(Y_i \\mid X_i). $$\n",
    "\n",
    "The term $\\mathcal{L}_1$ does not involve the parameters $\\beta_0$ and $\\beta_1$. We shall focus on the second term $\\mathcal{L}_2$, which is called the **conditional likelihood**, given by\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_2 \\equiv \\mathcal{L}(\\beta_0, \\beta_1, \\sigma) = \\prod_{i=1}^n f_{Y \\mid X}(Y_i \\mid X_i) \\propto \\sigma^{-n} \\exp \\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{i} (Y_i - \\mu_i)^2 \\right\\}.\n",
    "$$\n",
    "\n",
    "The conditional log-likelihood is\n",
    "\n",
    "$$ \\ell(\\beta_0, \\beta_1, \\sigma) = -n \\log \\sigma - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\Big(Y_i - (\\beta_0 + \\beta_1 X_i)\\Big)^2. $$\n",
    "\n",
    "To find the MLE of $(\\beta_0, \\beta_1)$, we maximize $\\ell(\\beta_0, \\beta_1, \\sigma)$. From the equation above, we see that maximizing the likelihood is the same as minimizing the RSS: $\\sum_{i=1}^n \\Big(Y_i - (\\beta_0 + \\beta_1 X_i)\\Big)^2.$ Therefore, we have shown that under the assumption of normality, the least squares estimator is also the maximum likelihood estimator.\n",
    "\n",
    "We can also maximize $\\ell(\\beta_0, \\beta_1, \\sigma)$ over $\\sigma$, yielding the MLE\n",
    "\n",
    "$$ \\widehat{\\sigma}^2 = \\frac{1}{n} \\sum_{i} \\widehat{\\epsilon}_i^2. $$\n",
    "\n",
    "This estimator is similar to, but not identical to, the unbiased estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True β0 = 2, Estimated β0 = 1.707\n",
      "True β1 = 3, Estimated β1 = 3.040\n",
      "Residual Sum of Squares (RSS) = 80.228\n",
      "True σ² = 1, MLE estimate of σ² = 0.802\n"
     ]
    }
   ],
   "source": [
    "# Define the negative log-likelihood function\n",
    "def negative_log_likelihood(params, X, Y):\n",
    "    beta0, beta1, sigma = params\n",
    "    mu = beta0 + beta1 * X\n",
    "    n = len(Y)\n",
    "    resids = Y - mu\n",
    "    log_likelihood = -n * np.log(sigma) - (1 / (2 * sigma**2)) * np.sum(resids**2)\n",
    "    return -log_likelihood\n",
    "\n",
    "# Optimize the negative log-likelihood function\n",
    "result = opt.minimize(negative_log_likelihood, x0=[0, 0, 1], args=(X, Y), \n",
    "                      bounds=[(None, None), (None, None), (1e-10, None)])\n",
    "mle_beta0, mle_beta1, mle_sigma = result.x\n",
    "\n",
    "# Compute fitted values and residuals\n",
    "mle_Y = mle_beta0 + mle_beta1 * X\n",
    "mle_resids = Y - mle_Y\n",
    "mle_RSS = np.sum(mle_resids**2)\n",
    "mle_var_hat = (1 / n) * mle_RSS\n",
    "\n",
    "# Print results\n",
    "print(f\"True β0 = {true_beta0}, Estimated β0 = {mle_beta0:.3f}\")\n",
    "print(f\"True β1 = {true_beta1}, Estimated β1 = {mle_beta1:.3f}\")\n",
    "print(f\"Residual Sum of Squares (RSS) = {mle_RSS:.3f}\")\n",
    "print(f\"True σ² = {true_var}, MLE estimate of σ² = {mle_var_hat:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Properties of the Least Squares Estimators\n",
    "\n",
    "We now record the standard errors and limiting distribution of the least squares estimator. In regression problems, we usually focus on the properties of the estimators conditional on $X^n = (X_1, \\ldots, X_n)$. Thus, we state the means and variances as conditional means and variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\widehat{\\beta}_T = (\\widehat{\\beta}_0, \\widehat{\\beta}_1)^T$ denote the least squares estimators. Then,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\widehat{\\beta} \\mid X^n) = \\begin{pmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{V}(\\widehat{\\beta} \\mid X^n) = \\frac{\\sigma^2}{n \\, s^2_{X}} \\begin{pmatrix}\n",
    "\\frac{1}{n} \\sum_{i=1}^n X_i^2 & -\\bar{X}_n \\\\\n",
    "-\\bar{X}_n & 1\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $s_X^2 = n^{-1} \\sum_{i=1}^n (X_i - \\bar{X}_n)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated standard errors of $\\widehat{\\beta}_0$ and $\\widehat{\\beta}_1$ are obtained by taking the square roots of the corresponding diagonal terms of $\\mathbb{V}(\\widehat{\\beta} \\mid X_n)$ and inserting the estimate $\\widehat{\\sigma}$ for $\\sigma$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Error of β0: 0.170\n",
      "Standard Error of β1: 0.031\n"
     ]
    }
   ],
   "source": [
    "# Compute standard errors\n",
    "s_X2 = np.mean((X - X_mean)**2)\n",
    "var_cov_matrix = (ols_var_hat / (n * s_X2)) * np.array([[np.mean(X**2), -X_mean], [-X_mean, 1]])\n",
    "se_ols_beta0 = np.sqrt(var_cov_matrix[0, 0])\n",
    "se_ols_beta1 = np.sqrt(var_cov_matrix[1, 1])\n",
    "\n",
    "# Print results\n",
    "print(f\"Standard Error of β0: {se_ols_beta0:.3f}\")\n",
    "print(f\"Standard Error of β1: {se_ols_beta1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.990</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.990</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   9893.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 11 Sep 2024</td> <th>  Prob (F-statistic):</th> <td>3.14e-100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:17:24</td>     <th>  Log-Likelihood:    </th> <td> -130.88</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   265.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   271.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    1.7066</td> <td>    0.170</td> <td>   10.049</td> <td> 0.000</td> <td>    1.370</td> <td>    2.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    3.0403</td> <td>    0.031</td> <td>   99.463</td> <td> 0.000</td> <td>    2.980</td> <td>    3.101</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.421</td> <th>  Durbin-Watson:     </th> <td>   1.983</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.810</td> <th>  Jarque-Bera (JB):  </th> <td>   0.376</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.146</td> <th>  Prob(JB):          </th> <td>   0.829</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.926</td> <th>  Cond. No.          </th> <td>    10.7</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.990   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.990   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     9893.   \\\\\n",
       "\\textbf{Date:}             & Wed, 11 Sep 2024 & \\textbf{  Prob (F-statistic):} & 3.14e-100   \\\\\n",
       "\\textbf{Time:}             &     15:17:24     & \\textbf{  Log-Likelihood:    } &   -130.88   \\\\\n",
       "\\textbf{No. Observations:} &         100      & \\textbf{  AIC:               } &     265.8   \\\\\n",
       "\\textbf{Df Residuals:}     &          98      & \\textbf{  BIC:               } &     271.0   \\\\\n",
       "\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &       1.7066  &        0.170     &    10.049  &         0.000        &        1.370    &        2.044     \\\\\n",
       "\\textbf{x1}    &       3.0403  &        0.031     &    99.463  &         0.000        &        2.980    &        3.101     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  0.421 & \\textbf{  Durbin-Watson:     } &    1.983  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.810 & \\textbf{  Jarque-Bera (JB):  } &    0.376  \\\\\n",
       "\\textbf{Skew:}          & -0.146 & \\textbf{  Prob(JB):          } &    0.829  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.926 & \\textbf{  Cond. No.          } &     10.7  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.990\n",
       "Model:                            OLS   Adj. R-squared:                  0.990\n",
       "Method:                 Least Squares   F-statistic:                     9893.\n",
       "Date:                Wed, 11 Sep 2024   Prob (F-statistic):          3.14e-100\n",
       "Time:                        15:17:24   Log-Likelihood:                -130.88\n",
       "No. Observations:                 100   AIC:                             265.8\n",
       "Df Residuals:                      98   BIC:                             271.0\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          1.7066      0.170     10.049      0.000       1.370       2.044\n",
       "x1             3.0403      0.031     99.463      0.000       2.980       3.101\n",
       "==============================================================================\n",
       "Omnibus:                        0.421   Durbin-Watson:                   1.983\n",
       "Prob(Omnibus):                  0.810   Jarque-Bera (JB):                0.376\n",
       "Skew:                          -0.146   Prob(JB):                        0.829\n",
       "Kurtosis:                       2.926   Cond. No.                         10.7\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm our results using statsmodels.api.OLS \n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_with_const = sm.add_constant(X)\n",
    "ols = sm.OLS(Y, X_with_const)\n",
    "ols_result = ols.fit()\n",
    "ols_result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction\n",
    "\n",
    "Suppose we have estimated a regression model $\\widehat{r}(x) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x$ from data $(X_1, Y_1), \\ldots, (X_n, Y_n)$. We observe the value $X = x_*$ of the covariate for a new subject and we want to predict their outcome $Y_*$. An estimate of $Y_*$ is\n",
    "\n",
    "$$ \\widehat{Y}_* = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_*. $$\n",
    "\n",
    "Using the formula for the variance of the sum of two random variables,\n",
    "\n",
    "$$\n",
    "\\mathbb{V}(\\widehat{Y}_*) = \\mathbb{V}(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_*) = \\mathbb{V}(\\widehat{\\beta}_0) + x_*^2 \\mathbb{V}(\\widehat{\\beta}_1) + 2 x_* \\, \\text{Cov}(\\widehat{\\beta}_0, \\widehat{\\beta}_1).\n",
    "$$\n",
    "\n",
    "The estimated standard error $\\text{se}(\\widehat{Y}_*)$ is the square root of this variance, with $\\widehat{\\sigma}^2$ in place of $\\sigma^2$. However, the confidence interval for $Y_*$ is **not** of the usual form $\\widehat{Y}_* \\pm z_{\\alpha/2} \\text{se}(\\widehat{Y}_*)$. The correct form of the confidence interval is given in the following theorem.\n",
    "\n",
    "Let\n",
    "\n",
    "$$ \\hat{\\xi}_n^2 = \\widehat{\\sigma}^2 \\left( \\frac{\\sum_{i=1}^n (X_i - X_*)^2}{n \\sum_{i} (X_i - \\bar{X})^2} + 1 \\right). $$\n",
    "\n",
    "An approximate $1 - \\alpha$ prediction interval for $Y_*$ is\n",
    "\n",
    "$$ \\widehat{Y}_{*} \\pm z_{\\alpha/2} \\, \\hat{\\xi}_{n}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Y = 22.989 for x = 7\n",
      "95% Prediction Interval for Y = (21.201, 24.776)\n"
     ]
    }
   ],
   "source": [
    "# Compute prediction for a new x value\n",
    "x_star = 7\n",
    "Y_star = ols_beta0 + ols_beta1 * x_star\n",
    "xi_n_squared = ols_var_hat * ((np.sum((X - x_star)**2) / (n * np.sum((X - X_mean)**2))) + 1)\n",
    "se_Y_star = np.sqrt(xi_n_squared)\n",
    "alpha = 0.05\n",
    "z_alpha = ss.norm.ppf(1 - alpha / 2)\n",
    "pred_int = (Y_star - z_alpha * se_Y_star, Y_star + z_alpha * se_Y_star)\n",
    "\n",
    "print(f\"Predicted Y = {Y_star:.3f} for x = {x_star}\")\n",
    "print(f\"95% Prediction Interval for Y = ({pred_int[0]:.3f}, {pred_int[1]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multiple Regression\n",
    "\n",
    "Now suppose that the covariate is a vector of length $k$. The data are of the form\n",
    "\n",
    "$$ (Y_1, X_1), \\ldots, (Y_i, X_i), \\ldots, (Y_n, X_n) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ X_i = (X_{i1}, \\ldots, X_{ik}). $$\n",
    "\n",
    "Here, $X_i$ is the vector of $k$ covariate values for the $i^\\text{th}$ observation. The linear regression model is\n",
    "\n",
    "$$ Y_i = \\sum_{j=1}^k \\beta_j X_{ij} + \\epsilon_i $$\n",
    "\n",
    "for $i = 1, \\ldots, n$, where $\\mathbb{E}(\\epsilon_i \\mid X_{i1}, \\ldots, X_{ik}) = 0$. Usually, we want to include an intercept in the model, which we can do by setting $X_{i1} = 1$ for $i = 1, \\ldots, n$. At this point, it will be more convenient to express the model in matrix notation. The outcomes will be denoted by\n",
    "\n",
    "$$\n",
    "Y = \n",
    "\\begin{pmatrix}\n",
    "Y_1 \\\\\n",
    "Y_2 \\\\\n",
    "\\vdots \\\\\n",
    "Y_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and the covariates will be denoted by\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "X_{11} & X_{12} & \\cdots & X_{1k} \\\\\n",
    "X_{21} & X_{22} & \\cdots & X_{2k} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "X_{n1} & X_{n2} & \\cdots & X_{nk}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Each row is one observation; the columns correspond to the $k$ covariates. Thus, $X$ is a $(n \\times k)$ matrix. Let\n",
    "\n",
    "$$\n",
    "\\beta = \n",
    "\\begin{pmatrix}\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_k\n",
    "\\end{pmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "\\epsilon = \n",
    "\\begin{pmatrix}\n",
    "\\epsilon_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_n\n",
    "\\end{pmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can write\n",
    "\n",
    "$$ Y = X \\beta + \\epsilon. $$\n",
    "\n",
    "The form of the least squares estimate is given in the following theorem. Assuming that the $(k \\times k)$ matrix $X^TX$ is invertible, \n",
    "\n",
    "$$ \\widehat{\\beta} = (X^T X)^{-1} X^T Y \\quad $$\n",
    "\n",
    "$$ \\mathbb{V}(\\widehat{\\beta} \\mid X_n) = \\sigma^2 (X^T X)^{-1} \\quad $$\n",
    "\n",
    "$$ \\widehat{\\beta} \\approx N(\\beta, \\sigma^2 (X^T X)^{-1}). $$\n",
    "\n",
    "The estimated regression function is $\\widehat{r}(x) = \\sum_{j=1}^k \\widehat{\\beta}_j x_j$. An unbiased estimate of $\\sigma^2$ is\n",
    "\n",
    "$$ \\widehat{\\sigma}^2 = \\left(\\frac{1}{n - k}\\right) \\sum_{i=1}^n \\widehat{\\epsilon}_i^2 $$\n",
    "\n",
    "where $\\widehat{\\epsilon} = X \\widehat{\\beta} - Y$ is the vector of residuals. An approximate $1 - \\alpha$ confidence interval for $\\beta_j$ is\n",
    "\n",
    "$$ \\widehat{\\beta}_j \\pm z_{\\alpha/2} \\widehat{\\text{se}}(\\widehat{\\beta}_j) $$\n",
    "\n",
    "where $\\widehat{\\text{se}}^2(\\widehat{\\beta}_j)$ is the $j^\\text{th}$ diagonal element of the matrix $\\widehat{\\sigma}_{\\widehat{}}^2 (X^T X)^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Selection\n",
    "\n",
    "In model selection there are two problems: (i) assigning a \"score\" to each model which measures, in some sense, how good the model is, and (ii) searching through all the models to find the model with the best score.\n",
    "\n",
    "The idea of **stepwise regression** is to build a regression model by adding/removing predictors step-by-step, until the pre-set significance level is met for all predictors. Each step in the stepwise regression procedure can be identified as \"forward selection\" or \"backward selection\", which mean adding a predictor or removing a existing predictor respectively.\n",
    "\n",
    "**AIC (Akaike Information Criterion)** and **BIC (Bayesian information criterion)** are two important examples of criterion-based model selection approaches. AIC and BIC have two similar objective functions to minimize as shown below.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{AIC} &= n \\ln \\left(\\frac{\\text{RSS}}{n} \\right) + 2(k+1) \\\\\n",
    "\\text{BIC} &= n \\ln \\left(\\frac{\\text{RSS}}{n} \\right) + (k+1) \\ln n\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training data and $k$ is the number of covariates (or parameters). Both criteria help in model selection by balancing fit and complexity, but BIC generally imposes a stricter penalty for complexity, especially for larger sample sizes.\n",
    "\n",
    "Another method is **$\\mathbf{k}$-fold cross-validation**. Here we divide the data into $\\mathrm{k}$ groups; often people take $\\mathrm{k} = 10$. We omit one group of data and fit the models to the remaining data. We use the fitted model to predict the data in the group that was omitted. We then estimate the risk by $\\sum_i(Y_i − \\widehat{Y}_i)^2$ where the sum is over the the data points in the omitted group. This process is repeated for each of the $\\mathrm{k}$ groups and the resulting risk estimates are averaged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Logistic Regression\n",
    "\n",
    "So far we have assumed that $Y_i$ is real valued. **Logistic regression** is a parametric method for regression when $Y_i \\in \\{0, 1\\}$ is binary. For a $k$-dimensional covariate $X$, the model is\n",
    "\n",
    "$$ p_i \\equiv p_i(\\beta) \\equiv \\mathbb{P}(Y_i = 1 \\mid X = x) = \\frac{e^{\\beta_0 + \\sum_{j=1}^k \\beta_j x_{ij}}}{1 + e^{\\beta_0 + \\sum_{j=1}^k \\beta_j x_{ij}}} $$\n",
    "\n",
    "or, equivalently,\n",
    "\n",
    "$$ \\text{logit}(p_i) = \\beta_0 + \\sum_{j=1}^k \\beta_j x_{ij} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\text{logit}(p) = \\log \\left( \\frac{p}{1 - p} \\right). $$\n",
    "\n",
    "The name \"logistic regression\" comes from the fact that $e^x / (1 + e^x)$ is called the logistic function. A plot of the logistic for a one-dimensional covariate is shown below.\n",
    "\n",
    "<center><img src=\"../figures/logistic_reg.png\" width=\"400\" height=\"125\"/></center>\n",
    "\n",
    "Because the $Y_i$'s are binary, the data are Bernoulli:\n",
    "\n",
    "$$ Y_i \\mid X_i = x_i \\sim \\text{Bernoulli}(p_i). $$\n",
    "\n",
    "Hence the (conditional) likelihood function is\n",
    "\n",
    "$$ \\mathcal{L}(\\beta) = \\prod_{i=1}^n p_i(\\beta)^{Y_i} \\, (1 - p_i(\\beta))^{1 - Y_i}. $$\n",
    "\n",
    "The MLE $\\widehat{\\beta}$ has to be obtained by maximizing $\\mathcal{L}(\\beta)$ numerically. There is a fast numerical algorithm called reweighted least squares, the details of which are outlined in Wasserman (2004, pp. 224). The Fisher information matrix $I$ can also be obtained numerically. The estimated standard error of $\\widehat{\\beta}_j$ is the $(j, j)$ element of $J = I^{-1}$. Model selection is usually done using the AIC score $\\ell_S - |S|$ where $\\ell_S$ is the log-likelihood of the model evaluated at the MLE.\n",
    "\n",
    "Let's consider a simple example in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.514392\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  100\n",
      "Model:                          Logit   Df Residuals:                       96\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Wed, 11 Sep 2024   Pseudo R-squ.:                  0.2560\n",
      "Time:                        15:17:24   Log-Likelihood:                -51.439\n",
      "converged:                       True   LL-Null:                       -69.135\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.007e-07\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.1626      0.248     -0.657      0.511      -0.648       0.323\n",
      "x1             0.6313      0.313      2.020      0.043       0.019       1.244\n",
      "x2            -1.1376      0.295     -3.857      0.000      -1.716      -0.560\n",
      "x3             0.7880      0.265      2.969      0.003       0.268       1.308\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression code example\n",
    "n_samples, n_features = 100, 3\n",
    "X = ss.norm.rvs(size=(n_samples, n_features), random_state=42)\n",
    "true_beta = np.array([0.5, -1.0, 0.75])\n",
    "\n",
    "p = sm.families.links.Logit().inverse(X @ true_beta)              # compute probabilities using logistic function\n",
    "Y = ss.bernoulli.rvs(p, random_state=42)                          # generate binary outcomes using scipy.stats.bernoulli\n",
    "X_with_intercept = sm.add_constant(X)                             # add constant (intercept) to features matrix\n",
    "logit_model = sm.Logit(Y, X_with_intercept)                       # fit logistic regression model using statsmodels\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Print result\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard errors = [0.24771597 0.31258112 0.29495784 0.26537467]\n",
      "AIC = 110.878\n"
     ]
    }
   ],
   "source": [
    "# Compute the Fisher Information Matrix and its inverse\n",
    "fisher_info = result.cov_params()\n",
    "std_errors = np.sqrt(np.diag(fisher_info))\n",
    "aic = result.aic\n",
    "\n",
    "# Print results\n",
    "print(f\"Standard errors = {std_errors}\")\n",
    "print(f\"AIC = {aic:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
